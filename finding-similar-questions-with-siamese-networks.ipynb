{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Finding Similar Questions with Siamese Networks "},{"metadata":{},"cell_type":"markdown","source":"\n* [Introduction](#section-one)\n* [Data Preparation](#section-two)\n* [Model Selection and Creation](#section-three)\n    - [Model](#subsection-one)\n    - [Loss Function](#anything-you-like)\n* [Evaluation](#section-three)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n## Introduction\n\n\nThe objective of this project is to determine whether two questions are pairs or not. On websites like Quora or StackOverflow, it can be beneficial for user experience to link similar questions together. This allows for users to not only see more answers, but also find the answer faster if someone has already asked their question. \n\nIn this work, I will be using the Siamese Network technique with LSTM models. Through this notebook, I will explain each step of my process. Additionally, I would like to mention that I am doing this project based off of my NLP Certificate on Coursera. So some code, including that of the data generator will be used based on that work. "},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install trax\n\nimport os\nimport nltk\nimport trax\nfrom trax import layers as tl\nfrom trax.supervised import training\nfrom trax.fastmath import numpy as fastnp\nimport numpy as np\nimport pandas as pd\nimport random as rnd\nfrom sklearn.model_selection import train_test_split","execution_count":24,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: trax in /opt/conda/lib/python3.7/site-packages (1.3.6)\nRequirement already satisfied: jax in /opt/conda/lib/python3.7/site-packages (from trax) (0.2.6)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from trax) (1.4.1)\nRequirement already satisfied: tensorflow-datasets in /opt/conda/lib/python3.7/site-packages (from trax) (3.0.0)\nRequirement already satisfied: funcsigs in /opt/conda/lib/python3.7/site-packages (from trax) (1.0.2)\nRequirement already satisfied: tensorflow-text in /opt/conda/lib/python3.7/site-packages (from trax) (2.3.0)\nRequirement already satisfied: gym in /opt/conda/lib/python3.7/site-packages (from trax) (0.17.3)\nRequirement already satisfied: t5 in /opt/conda/lib/python3.7/site-packages (from trax) (0.7.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from trax) (1.18.5)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from trax) (1.14.0)\nRequirement already satisfied: jaxlib in /opt/conda/lib/python3.7/site-packages (from trax) (0.1.57)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from trax) (0.11.0)\nRequirement already satisfied: gin-config in /opt/conda/lib/python3.7/site-packages (from trax) (0.3.0)\nRequirement already satisfied: opt-einsum in /opt/conda/lib/python3.7/site-packages (from jax->trax) (3.3.0)\nRequirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (3.14.0)\nRequirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (2.3)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (1.11.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (4.45.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (2.23.0)\nRequirement already satisfied: attrs>=18.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (19.3.0)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (1.1.0)\nRequirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (0.25.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (0.3.3)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets->trax) (0.18.2)\nRequirement already satisfied: tensorflow<2.4,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-text->trax) (2.3.1)\nRequirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym->trax) (1.5.0)\nRequirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym->trax) (1.3.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from t5->trax) (0.23.2)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from t5->trax) (3.2.4)\nRequirement already satisfied: rouge-score in /opt/conda/lib/python3.7/site-packages (from t5->trax) (0.0.4)\nRequirement already satisfied: transformers>=2.7.0 in /opt/conda/lib/python3.7/site-packages (from t5->trax) (3.4.0)\nRequirement already satisfied: mesh-tensorflow[transformer]>=0.1.13 in /opt/conda/lib/python3.7/site-packages (from t5->trax) (0.1.17)\nRequirement already satisfied: babel in /opt/conda/lib/python3.7/site-packages (from t5->trax) (2.8.0)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from t5->trax) (1.6.0)\nRequirement already satisfied: tfds-nightly in /opt/conda/lib/python3.7/site-packages (from t5->trax) (4.1.0.dev202011200106)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from t5->trax) (0.1.94)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from t5->trax) (1.1.4)\nRequirement already satisfied: sacrebleu in /opt/conda/lib/python3.7/site-packages (from t5->trax) (1.4.14)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.7/site-packages (from jaxlib->trax) (1.12)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.25.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2020.11.8)\nRequirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.52.0)\nRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.10.0)\nRequirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.33.2)\nRequirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.1.2)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.34.2)\nRequirement already satisfied: tensorboard<3,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.4.0)\nRequirement already satisfied: google-pasta>=0.1.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.2.0)\nRequirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.3.3)\nRequirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.3.0)\nRequirement already satisfied: astunparse==1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.6.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->t5->trax) (2.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->t5->trax) (0.14.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers>=2.7.0->t5->trax) (20.1)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers>=2.7.0->t5->trax) (0.0.43)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers>=2.7.0->t5->trax) (2020.4.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=2.7.0->t5->trax) (3.0.10)\nRequirement already satisfied: tokenizers==0.9.2 in /opt/conda/lib/python3.7/site-packages (from transformers>=2.7.0->t5->trax) (0.9.2)\nRequirement already satisfied: pytz>=2015.7 in /opt/conda/lib/python3.7/site-packages (from babel->t5->trax) (2019.3)\nRequirement already satisfied: importlib-resources; python_version < \"3.9\" in /opt/conda/lib/python3.7/site-packages (from tfds-nightly->t5->trax) (3.3.0)\nRequirement already satisfied: typing-extensions; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from tfds-nightly->t5->trax) (3.7.4.1)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->t5->trax) (2.8.1)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from sacrebleu->t5->trax) (2.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (3.2.1)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.4.1)\n","name":"stdout"},{"output_type":"stream","text":"Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (46.1.3.post20200325)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.7.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.0.1)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.14.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers>=2.7.0->t5->trax) (2.4.7)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=2.7.0->t5->trax) (7.1.1)\nRequirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from importlib-resources; python_version < \"3.9\"->tfds-nightly->t5->trax) (3.1.0)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.2.0)\nRequirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (4.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.2.7)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (3.1.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (3.0.1)\nRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.4.8)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-two\"></a>\n## Data Preparation\n\n\nThe data from Quora is stored in a csv format with questions to compare found in the same row. The training data is also labeled with the tag is_duplicate, indicated whether or not the questions have the same meaning (0 - No, 1 - Yes). In this step, I will go read in the data, clean up the text data, split the data into a train and test set, and create data generators for both sets that can be read in by our model. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/quora-question-pairs/train.csv.zip')\ntest = pd.read_csv('/kaggle/input/quora-question-pairs/test.csv')","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train.head()","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"   id  qid1  qid2                                          question1  \\\n0   0     1     2  What is the step by step guide to invest in sh...   \n1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n2   2     5     6  How can I increase the speed of my internet co...   \n3   3     7     8  Why am I mentally very lonely? How can I solve...   \n4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n\n                                           question2  is_duplicate  \n0  What is the step by step guide to invest in sh...             0  \n1  What would happen if the Indian government sto...             0  \n2  How can Internet speed be increased by hacking...             0  \n3  Find the remainder when [math]23^{24}[/math] i...             0  \n4            Which fish would survive in salt water?             0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>What is the step by step guide to invest in sh...</td>\n      <td>What is the step by step guide to invest in sh...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>3</td>\n      <td>4</td>\n      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n      <td>What would happen if the Indian government sto...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>5</td>\n      <td>6</td>\n      <td>How can I increase the speed of my internet co...</td>\n      <td>How can Internet speed be increased by hacking...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>7</td>\n      <td>8</td>\n      <td>Why am I mentally very lonely? How can I solve...</td>\n      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>9</td>\n      <td>10</td>\n      <td>Which one dissolve in water quikly sugar, salt...</td>\n      <td>Which fish would survive in salt water?</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"I am just randomly splitting the dataset and then "},{"metadata":{"trusted":true},"cell_type":"code","source":"N_train = 300000\nN_test  = 10*1024\ndata_train = train[:N_train]\ndata_test  = train[N_train:N_train+N_test]\nprint(\"Train set:\", len(data_train), \"Test set:\", len(data_test))\ndel(train) # remove to free memory","execution_count":27,"outputs":[{"output_type":"stream","text":"Train set: 300000 Test set: 10240\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"td_index = (data_train['is_duplicate'] == 1).to_numpy()\ntd_index = [i for i, x in enumerate(td_index) if x] \n\nQ1_train_words = np.array(data_train['question1'][td_index])\nQ2_train_words = np.array(data_train['question2'][td_index])\n\nQ1_test_words = np.array(data_test['question1'])\nQ2_test_words = np.array(data_test['question2'])\ny_test  = np.array(data_test['is_duplicate'])","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create arrays\nQ1_train = np.empty_like(Q1_train_words)\nQ2_train = np.empty_like(Q2_train_words)\n\nQ1_test = np.empty_like(Q1_test_words)\nQ2_test = np.empty_like(Q2_test_words)","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building the vocabulary with the train set         \nfrom collections import defaultdict\n\nvocab = defaultdict(lambda: 0)\nvocab['<PAD>'] = 1\n\nfor idx in range(len(Q1_train_words)):\n    Q1_train[idx] = nltk.word_tokenize(Q1_train_words[idx])\n    Q2_train[idx] = nltk.word_tokenize(Q2_train_words[idx])\n    q = Q1_train[idx] + Q2_train[idx]\n    for word in q:\n        if word not in vocab:\n            vocab[word] = len(vocab) + 1\nprint('The length of the vocabulary is: ', len(vocab))","execution_count":30,"outputs":[{"output_type":"stream","text":"The length of the vocabulary is:  36352\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx in range(len(Q1_test_words)): \n    Q1_test[idx] = nltk.word_tokenize(Q1_test_words[idx])\n    Q2_test[idx] = nltk.word_tokenize(Q2_test_words[idx])","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train set has reduced to: ', len(Q1_train) ) \nprint('Test set length: ', len(Q1_test) )","execution_count":32,"outputs":[{"output_type":"stream","text":"Train set has reduced to:  111473\nTest set length:  10240\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting the data\ncut_off = int(len(Q1_train)*.8)\ntrain_Q1, train_Q2 = Q1_train[:cut_off], Q2_train[:cut_off]\nval_Q1, val_Q2 = Q1_train[cut_off: ], Q2_train[cut_off:]\nprint('Number of duplicate questions: ', len(Q1_train))\nprint(\"The length of the training set is:  \", len(train_Q1))\nprint(\"The length of the validation set is: \", len(val_Q1))","execution_count":33,"outputs":[{"output_type":"stream","text":"Number of duplicate questions:  111473\nThe length of the training set is:   89178\nThe length of the validation set is:  22295\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_generator(Q1, Q2, batch_size, pad=1, shuffle=True):\n    \"\"\"Generator function that yields batches of data\n    \"\"\"\n\n    input1 = []\n    input2 = []\n    idx = 0\n    len_q = len(Q1)\n    question_indexes = [*range(len_q)]\n    \n    if shuffle:\n        rnd.shuffle(question_indexes)\n    \n    while True:\n        if idx >= len_q:\n            idx = 0\n            # shuffle to get random batches if shuffle is set to True\n            if shuffle:\n                rnd.shuffle(question_indexes)\n        \n        q1 = Q1[question_indexes[idx]]\n        q2 = Q2[question_indexes[idx]]\n\n        idx += 1\n        input1.append(q1)\n        input2.append(q2)\n        if len(input1) == batch_size:\n            max_len = max(max([len(_) for _ in input1]),max([len(_) for _ in input2]))\n            max_len = 2**int(np.ceil(np.log2(max_len)))\n            b1 = []\n            b2 = []\n            for q1, q2 in zip(input1, input2):\n                q1 = q1 + [pad] * (max_len - len(q1))\n                q2 = q2 + [pad] * (max_len - len(q2))\n               \n                b1.append(q1)\n                b2.append(q2)\n\n            yield np.array(b1), np.array(b2)\n\n            # reset the batches\n            input1, input2 = [], []  # reset the batches","execution_count":34,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Siamese Model Creation\n\nA siamese model works to create two or more identical subnetworks. It may seem counterintuitive to train the same network twice, as it will take quite literally double the computation time. However, in this use case of similarity comparison this network architecture makes a lot of sense. In short terms, we will be created a dual layered subnetwork that will work to compare two questions. Each question will run through a different subnetwork that will use mathematical operations (or ML magic) to break down each question into a vector that contains it's meaning. These two vectors containing the intuition about the questions will be compared using cosine similarity. This metric will return a score from -1 to 1, where -1 means that the questions are very different and 1 shows that they have the meaning. \n\nAn important step of this model is to build the network architectures of the submodels. In this work, I used Long Short Term Memory (LSTM) models to do the heavy lifting. RNN models were popularized by working with text models. However, they tend to struggle with longer sequences and are prone to vanishing gradients. LSTM models work better at capturing important aspects of text sequences as they learn to remember and when to forget. If the most important part of a sequence is in the beginning, traditional RNN models put less and less emphasis on the beginning chunks of a sequence as the sequence grows in length. LSTM remembers the important parts of the sequence and forgets the unimportant. "},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize(x):\n    return x / np.sqrt(np.sum(x * x, axis=-1, keepdims=True))","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the Siamese Model\n\ndef Siamese(vocab_size=len(vocab), model_dimension=128, mode='train'):\n    \"\"\"Returns a Siamese model.\n\n    Args:\n        vocab_size (int, optional): Length of the vocabulary. Defaults to len(vocab).\n        d_model (int, optional): Depth of the model. Defaults to 128.\n        mode (str, optional): 'train', 'eval' or 'predict', predict mode is for fast inference. Defaults to 'train'.\n\n    Returns:\n        trax.layers.combinators.Parallel: A Siamese model. \n    \"\"\"\n\n    def normalize(x):  # normalizes the vectors to have L2 norm 1\n        return x / fastnp.sqrt(fastnp.sum(x * x, axis=-1, keepdims=True))\n    \n    #create the LSTM Model that makes up the backbone of our Siamese Model\n    LSTM = tl.Serial(\n        tl.Embedding(vocab_size=vocab_size, d_feature=model_dimension),\n        tl.LSTM(model_dimension),\n        tl.Mean(axis=1),\n        tl.Fn('Normalize', lambda x: normalize(x))\n    )\n    \n    # Run on Q1 and Q2 in parallel.\n    model = tl.Parallel(LSTM, LSTM)\n    return model","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check your model\nmodel = Siamese()\nprint(model)","execution_count":44,"outputs":[{"output_type":"stream","text":"Parallel_in2_out2[\n  Serial[\n    Embedding_36352_128\n    LSTM_128\n    Mean\n    Normalize\n  ]\n  Serial[\n    Embedding_36352_128\n    LSTM_128\n    Mean\n    Normalize\n  ]\n]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"LSTM Architecture Step by Step:\n1. Forget Gate : Sigmoid Function (0 throw information out, 1 keep)\n2. Input Gate: updates the current cell state\n* *     Sigmoid Layer: closer to 1 = more important keep\n* *  Tanh Layer: -1 to 1 helps regulate the flow of informatin\n* * Outputs Sigmoid * Tanh layers for usable state values\n3. Output Gate: what next hidden state should be"},{"metadata":{},"cell_type":"markdown","source":"### Loss Function"},{"metadata":{"trusted":true},"cell_type":"code","source":"def TripletLossFn(v1, v2, margin=0.25):\n    \"\"\"\n    Custom Triplet Loss Function\n    \"\"\"\n    scores = fastnp.dot(v1,v2.T) # pairwise cosine sim\n\n    batch_size = len(scores)\n    \n    positive = fastnp.diagonal(scores)  # the positive ones (duplicates)\n\n    negative_without_positive = scores - fastnp.eye(batch_size) * 2.0 \n    \n    closest_negative = negative_without_positive.max(axis = 1)\n    \n    negative_zero_on_duplicate = (1.0 - fastnp.eye(batch_size)) * scores\n    \n    mean_negative = fastnp.sum(negative_zero_on_duplicate, axis=1) / (batch_size - 1)\n    \n    triplet_loss1 = fastnp.maximum(margin - positive + closest_negative, 0 )\n    triplet_loss2 = fastnp.maximum(margin - positive + mean_negative, 0 )\n    triplet_loss = fastnp.mean(triplet_loss1 + triplet_loss2)\n\n    \n    return triplet_loss","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Make Triplet Loss Layer\n\nfrom functools import partial\ndef TripletLoss(margin=0.25):\n    triplet_loss_fn = partial(TripletLossFn, margin=margin)\n    return tl.Fn('TripletLoss', triplet_loss_fn)","execution_count":38,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 256\ntrain_generator = data_generator(train_Q1, train_Q2, batch_size, vocab['<PAD>'])\nval_generator = data_generator(val_Q1, val_Q2, batch_size, vocab['<PAD>'])\nprint('train_Q1.shape ', train_Q1.shape)\nprint('val_Q1.shape   ', val_Q1.shape)","execution_count":39,"outputs":[{"output_type":"stream","text":"train_Q1.shape  (89178,)\nval_Q1.shape    (22295,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_schedule = trax.lr.warmup_and_rsqrt_decay(400, 0.01)\n\ndef train_model(Siamese, TripletLoss, lr_schedule, train_generator=train_generator, val_generator=val_generator, output_dir='model/'):\n    \n    output_dir = os.path.expanduser(output_dir)\n\n    train_task = training.TrainTask(\n        labeled_data=train_generator,       # Use generator (train)\n        loss_layer=TripletLoss(),         # Use triplet loss. Don't forget to instantiate this object\n        optimizer=trax.optimizers.Adam(learning_rate = 0.01),          # Don't forget to add the learning rate parameter\n        lr_schedule=lr_schedule, # Use Trax multifactor schedule function\n    )\n\n    eval_task2 = training.EvalTask(\n        labeled_data=val_generator,       # Use generator (val)\n        metrics=[TripletLoss()]         # Use triplet loss. Don't forget to instantiate this object\n    )\n\n\n    training_loop = training.Loop(Siamese(),\n                                  train_task,\n                                  eval_task=[eval_task2],\n                                  output_dir=output_dir)\n\n    return training_loop","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_steps = 5\ntraining_loop = train_model(Siamese, TripletLoss, lr_schedule)\ntraining_loop.run(train_steps)","execution_count":54,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"__init__() got an unexpected keyword argument 'eval_task'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-54-b8ba7e43cd69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSiamese\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTripletLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_schedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-53-bd094cdf0efb>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(Siamese, TripletLoss, lr_schedule, train_generator, val_generator, output_dir)\u001b[0m\n\u001b[1;32m     21\u001b[0m                                   \u001b[0mtrain_task\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                   \u001b[0meval_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meval_task2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                                   output_dir=output_dir)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'eval_task'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}